# 零拷贝技术

参考文章：

 [深入剖析Linux IO原理和几种零拷贝机制的实现](https://zhuanlan.zhihu.com/p/83398714)

前置知识: 

[内存概念](../../操作系统/linux/内存概念)

[内核空间vs用户空间](../../操作系统/linux/内核空间vs用户空间)

[Linux IO](../../操作系统/linux/Linux IO)

## **什么是拷贝**

要知道零拷贝，需要先知道什么是拷贝。其实这个”拷贝“来自于 Linux 中的标准 I/O 接口的基本数据拷贝的操作，

而这一操作会导致数据在**操作系统内核地址空间**的缓冲区和应用**程序地址空间**定义的缓冲区之间进行传输。



## 为什么需要零拷贝

为了更好的理解零拷贝解决的问题，我们首先了解一下传统 I/O 方式存在的问题。

在 Linux 系统中，传统的访问方式是通过 write() 和 read() 两个系统调用实现的，通过 read() 函数读取文件到到缓存区中，然后通过 write() 方法把缓存中的数据输出到网络端口，伪代码如下：

```cpp
read(file_fd, tmp_buf, len);
write(socket_fd, tmp_buf, len);
```

下图分别对应传统 I/O 操作的数据读写流程，整个过程涉及 2 次 CPU 拷贝、2 次 DMA 拷贝总共 4 次拷贝，以及 4 次上下文切换

![img](https://pic1.zhimg.com/80/v2-18e66cbb4e06d1f13e4335898c7b8e8c_720w.jpg)

- 上下文切换：当用户程序向内核发起系统调用时，CPU 将用户进程从用户态切换到内核态；当系统调用返回时，CPU 将用户进程从内核态切换回用户态。
- CPU拷贝：由 CPU 直接处理数据的传送，数据拷贝时会一直占用 CPU 的资源。
- DMA拷贝：由 CPU 向DMA磁盘控制器下达指令，让 DMA 控制器来处理数据的传送，数据传送完毕再把信息反馈给 CPU，从而减轻了 CPU 资源的占有率。



综上所述，因为有内核-用户 空间的传输，共产生了**四次数据拷贝**， 又在用户态与内核态也发生了**多次上下文切换**，严重影响效率

而零拷贝主要就是为了解决这种低效率



## 优化思路

在 Linux 中零拷贝技术主要有 3 个实现思路：**用户态直接 I/O**、**减少数据拷贝次数**以及**写时复制**技术。

- 用户态直接 I/O：应用程序可以直接访问硬件存储，操作系统内核只是辅助数据传输。这种方式依旧存在用户空间和内核空间的上下文切换，硬件上的数据直接拷贝至了用户空间，不经过内核空间。因此，直接 I/O 不存在内核空间缓冲区和用户空间缓冲区之间的数据拷贝。
- 减少数据拷贝次数：在数据传输过程中，避免数据在用户空间缓冲区和系统内核空间缓冲区之间的CPU拷贝，以及数据在系统内核空间内的CPU拷贝，这也是当前主流零拷贝技术的实现思路。
- 写时复制技术：写时复制指的是当多个进程共享同一块数据时，如果其中一个进程需要对这份数据进行修改，那么将其拷贝到自己的进程地址空间中，如果只是数据读取操作则不需要进行拷贝操作。



事实上，零拷贝的实现方式有很多，目的其实就是为了减少数据拷贝



## Linux实现方式

1. #### 直接I/O

   用户态直接 I/O 使得应用进程或运行在用户态（user space）下的库函数直接访问硬件设备，数据直接跨过内核进行传输，内核在数据传输过程除了进行必要的虚拟存储配置工作之外，不参与任何其他工作，这种方式能够直接绕过内核，极大提高了性能。

   ![img](https://pic1.zhimg.com/80/v2-4cb0f465ebeb7ff0f5e31e8d3f790c80_720w.jpg)

   缺点: 

   1. 用户态直接 I/O 只能适用于不需要内核缓冲区处理的应用程序，这些应用程序通常在进程地址空间有自己的数据缓存机制，称为自缓存应用程序，如数据库管理系统就是一个代表。

   2. 其次，这种零拷贝机制会直接操作磁盘 I/O，由于 CPU 和磁盘 I/O 之间的执行时间差距，会造成大量资源的浪费，解决方案是配合异步 I/O 使用。

   

2. #### mmap() + write

   mmap 是 Linux 提供的一种内存映射文件方法，即将一个进程的地址空间中的一段虚拟地址映射到磁盘文件地址。

   第2种零拷贝方式是使用 mmap + write 代替原来的 read + write 方式，减少了 1 次 CPU 拷贝操作。

   

   

   使用 mmap 的目的是将内核中读缓冲区（read buffer）的地址与用户空间的缓冲区（user buffer）进行映射，从而实现内核缓冲区与应用程序内存的共享，省去了将数据从内核读缓冲区（read buffer）拷贝到用户缓冲区（user buffer）的过程，然而内核读缓冲区（read buffer）仍需将数据到内核写缓冲区（socket buffer），大致的流程如下图所示：

   ![img](https://pic2.zhimg.com/80/v2-28463616753963ac9f189ce23a485e2d_720w.jpg)

   基于 mmap + write 系统调用的零拷贝方式，整个拷贝过程会发生 4 次上下文切换，1 次 CPU 拷贝和 2 次 DMA 拷贝，用户程序读写数据的流程如下：

   1. 用户进程通过 mmap() 函数向内核（kernel）发起系统调用，上下文从用户态（user space）切换为内核态（kernel space）。
   2. 将用户进程的内核空间的读缓冲区（read buffer）与用户空间的缓存区（user buffer）进行内存地址映射。
   3. CPU利用DMA控制器将数据从主存或硬盘拷贝到内核空间（kernel space）的读缓冲区（read buffer）。
   4. 上下文从内核态（kernel space）切换回用户态（user space），mmap 系统调用执行返回。
   5. 用户进程通过 write() 函数向内核（kernel）发起系统调用，上下文从用户态（user space）切换为内核态（kernel space）。
   6. CPU将读缓冲区（read buffer）中的数据拷贝到的网络缓冲区（socket buffer）。
   7. CPU利用DMA控制器将数据从网络缓冲区（socket buffer）拷贝到网卡进行数据传输。
   8. 上下文从内核态（kernel space）切换回用户态（user space），write 系统调用执行返回。

   **优点**：

   ​	mmap 主要的用处是提高 I/O 性能，特别是针对大文件。

   **缺点**：

   1. 对于小文件，内存映射文件反而会导致碎片空间的浪费，因为内存映射总是要对齐页边界，最小单位是 4 KB，一个 5 KB 的文件将会映射占用 8 KB 内存，也就会浪费 3 KB 内存。
   2. mmap 的拷贝虽然减少了 1 次拷贝，提升了效率，但也存在一些隐藏的问题。当 mmap 一个文件时，如果这个文件被另一个进程所截获，那么 write 系统调用会因为访问非法地址被 SIGBUS 信号终止，SIGBUS 默认会杀死进程并产生一个 coredump，服务器可能因此被终止。

   

3. #### sendfile()

   sendfile 系统调用在 Linux 内核版本 2.1 中被引入，目的是简化通过网络在两个通道之间进行的数据传输过程。

   sendfile 系统调用的引入，不仅减少了 CPU 拷贝的次数，还减少了上下文切换的次数

   

   通过 sendfile 系统调用，数据可以直接在**内核空间内部**进行 I/O 传输，从而省去了数据在用户空间和内核空间之间的**来回拷贝**。

   与 mmap 内存映射方式不同的是， sendfile 调用中 I/O 数据对用户空间是完全不可见的。也就是说，这是一次完全意义上的数据传输过程。

   ![img](https://pic3.zhimg.com/80/v2-48132735369375701f3d8ac1d6029c2a_720w.jpg)

   基于 sendfile 系统调用的零拷贝方式，整个拷贝过程会发生 2 次上下文切换，1 次 CPU 拷贝和 2 次 DMA 拷贝，用户程序读写数据的流程如下：

   1. 用户进程通过 sendfile() 函数向内核（kernel）发起系统调用，上下文从用户态（user space）切换为内核态（kernel space）。
   2. CPU 利用 DMA 控制器将数据从主存或硬盘拷贝到内核空间（kernel space）的读缓冲区（read buffer）。
   3. CPU 将读缓冲区（read buffer）中的数据拷贝到的网络缓冲区（socket buffer）。
   4. CPU 利用 DMA 控制器将数据从网络缓冲区（socket buffer）拷贝到网卡进行数据传输。
   5. 上下文从内核态（kernel space）切换回用户态（user space），sendfile 系统调用执行返回。

   

   缺点: 

   1. 相比较于 mmap 内存映射的方式，sendfile 少了 2 次上下文切换，但是**仍然有 1 次 CPU 拷贝操作**。
   2. sendfile 存在的问题是用户程序**不能对数据进行修改**，而只是单纯地完成了一次数据传输过程。

   

4. 带有DMA收集拷贝功能的sendfile()

   之前我们是把页缓存的数据拷贝到socket缓存中，

   实际上，我们仅仅需要把缓冲区描述符传到 socket 缓冲区，再把数据长度传过去，这样 DMA 控制器直接将页缓存中的数据打包发送到网络中就可以了。

   ![img](https://pic1.zhimg.com/80/v2-244852829e9b22802b2004a22bb7eb18_720w.jpg)

   上图总结为以下 3 步：

   - DMA 从拷贝至内核缓冲区

   - 将数据的位置和长度的信息的描述符增加至内核空间（socket 缓冲区）

   - DMA 将数据从内核拷贝至协议引擎

     

5. splice()

   sendfile 利用了Linux提出的管道缓冲区机制，只适用于将数据从文件拷贝到套接字上，有一方必须是管道设备，因而限定了它的使用范围。





## 零拷贝实现方式の对比

无论是传统 I/O 拷贝方式还是引入零拷贝的方式，2 次 DMA Copy 是都少不了的，因为两次 DMA 都是依赖硬件完成的。

下面从 CPU 拷贝次数、DMA 拷贝次数以及系统调用几个方面总结一下上述几种 I/O 拷贝方式的差别。



![img](https://pic1.zhimg.com/80/v2-a728f1a35f62ba608c7eef7c43478edc_720w.jpg)



